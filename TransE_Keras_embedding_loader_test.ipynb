{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import keras\n",
    "from functools import partial\n",
    "from keras import layers\n",
    "from keras.layers import merge\n",
    "from keras import optimizers \n",
    "from keras import backend as K\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Functions for the training/test model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_init(k,shape, dtype=None):\n",
    "    unnormed = K.random_uniform(shape=shape, minval=-6/np.sqrt(k), maxval=6/np.sqrt(k), dtype=dtype)\n",
    "    norm     = K.sqrt(K.sum(K.pow(unnormed,2), axis=-1, keepdims=True))  \n",
    "    return unnormed/norm\n",
    "\n",
    "def L2_norm(x, keepdims=True):\n",
    "    return K.sqrt(K.sum(K.pow(x,2), axis=-1, keepdims=keepdims))\n",
    "\n",
    "def L1_norm(x, keepdims=True):\n",
    "    return K.sum(K.abs(x), axis=-1, keepdims=keepdims)\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):  ## y_true is unused, but keras \"fit\" method requires it, so when calling\n",
    "    return K.sum(y_pred)            ## the \"fit\" method also the y argument must be passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset manipulation functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexed_dataset(dataset, entities, rel):\n",
    "    #substituting the entities and the relationships in the dataset with their respective index in the \n",
    "    #entities dataframe and relationships dataframe\n",
    "\n",
    "    index_dataset=pd.merge(dataset, entities.rename(columns={\"entity\": \"h\"}), \n",
    "                           on='h', left_index=True).drop('h',1).rename(columns={\"index\": \"h\"})\n",
    "    index_dataset=pd.merge(index_dataset, entities.rename(columns={\"entity\": \"t\"}), \n",
    "                           on='t', left_index=True).drop('t',1).rename(columns={\"index\": \"t\"})\n",
    "    index_dataset=pd.merge(index_dataset, rel.rename(columns={\"relationship\": \"l\"}), \n",
    "                           on='l', left_index=True).drop('l',1).rename(\n",
    "                           columns={\"index\": \"l\"}).reset_index().drop('index',1).reset_index()\n",
    "    return index_dataset\n",
    "\n",
    "\n",
    "def generate_pos_neg_set(index_dataset):\n",
    "   \n",
    "    Negative=index_dataset.copy()\n",
    "\n",
    "    entities_size=np.shape(model.layers[2].get_weights()[0])[0]\n",
    "    #generate negative triplets\n",
    "\n",
    "    Negative_h,Negative_t=np.split(shuffle(Negative, random_state=np.random.randint(0,10000)), 2)\n",
    "    Negative_h['h']=Negative_h['h'].apply(lambda x: np.random.randint(0,entities_size))\n",
    "    Negative_t['t']=Negative_t['t'].apply(lambda x: np.random.randint(0,entities_size))\n",
    "    Negative=pd.concat((Negative_h, Negative_t))\n",
    "\n",
    "    #remove fake negative triplets\n",
    "    Negative = pd.merge(index_dataset.drop('index',1), Negative, on=['h', 't', 'l'], how='right', indicator='Exist')\n",
    "    Negative = Negative.drop(Negative[Negative['Exist']=='both'].index, axis=0).drop('Exist',1).rename(columns={\"h\": \"h1\", \"t\": \"t1\"})\n",
    "\n",
    "    #merge positive and negative triplets\n",
    "    Total=pd.merge(index_dataset, Negative, on='index').drop(['l_y','index'],1).rename(columns={\"l_x\": \"l\"})\n",
    "\n",
    "    Entities_set=Total.drop(['l'],1).to_numpy()\n",
    "    Rel_set=Total['l'].to_numpy()\n",
    "    n=np.shape(Rel_set)[0]\n",
    "\n",
    "    e=Entities_set.reshape(n,4)\n",
    "    l=Rel_set.reshape(n,1)\n",
    "    \n",
    "    return [e,l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_creation(rel_set, entities_set, k, norm_type='L2', optimizer='sgd'):\n",
    "\n",
    "    #get the sizes of the sets of unique relationships and entities\n",
    "    \n",
    "    rel_size=np.shape(rel_set)[0]\n",
    "    entities_size=np.shape(entities_set)[0]\n",
    "    \n",
    "    #Define initializers for the embedings\n",
    "    \n",
    "    init=keras.initializers.RandomUniform(minval=-6/np.sqrt(k), maxval=6/np.sqrt(k))\n",
    "    rel_initializer=partial(rel_init,k)\n",
    "    \n",
    "    #Define norm constraint on entities' embeddings\n",
    "    norm=keras.constraints.UnitNorm(axis=1)\n",
    "\n",
    "    #Define input shape\n",
    "    in_e=keras.Input((4,))\n",
    "    in_r=keras.Input((1,))\n",
    "\n",
    "    #Define embedding layers\n",
    "    embedding_e = layers.Embedding(entities_size, k, input_length=4, embeddings_initializer=init, embeddings_constraint=norm)(in_e)\n",
    "    embedding_r = layers.Embedding(rel_size, k, input_length=1, embeddings_initializer=rel_initializer)(in_r)\n",
    "\n",
    "    #Concatenate embedding layers into one layer\n",
    "    embedding   = layers.Concatenate(axis=1)([embedding_e,embedding_r])\n",
    "\n",
    "    #Find the values for the triplets (h,l,t) (positive) and (h1,l,t1) (negative)\n",
    "    \n",
    "    h           = layers.Lambda( lambda y: y[:,0,:])(embedding)\n",
    "    t           = layers.Lambda( lambda y: y[:,1,:])(embedding)\n",
    "    h1          = layers.Lambda( lambda y: y[:,2,:])(embedding)\n",
    "    t1          = layers.Lambda( lambda y: y[:,3,:])(embedding)\n",
    "    l           = layers.Lambda( lambda y: y[:,4,:])(embedding)\n",
    "\n",
    "    pos         = merge.subtract([merge.add([h,l]),t])\n",
    "    neg         = merge.subtract([merge.add([h1,l]),t1])\n",
    "\n",
    "    #Compute the dissimilarities/energies of the two triplets...\n",
    "\n",
    "    if norm_type=='L2':\n",
    "        pos_mid     = layers.Lambda(L2_norm)(pos) #L2 norm\n",
    "        neg_mid     = layers.Lambda(L2_norm)(neg) #L2 norm\n",
    "\n",
    "    elif norm_type=='L1':\n",
    "        pos_mid     = layers.Lambda(L1_norm)(pos) #L1 norm\n",
    "        neg_mid     = layers.Lambda(L1_norm)(neg) #L1 norm\n",
    "\n",
    "    #...then subtract them\n",
    "    \n",
    "    sub         = merge.subtract([pos_mid,neg_mid])\n",
    "\n",
    "    #finally compute [\\gamma+d(positive_triplet)-d(negative_triplet)]_+\n",
    "    \n",
    "    out         = layers.Lambda(lambda y: K.maximum(y+gamma,0))(sub)\n",
    "\n",
    "    \n",
    "    model=keras.Model([in_e,in_r], out)\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function for test model definition and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The function performs prediction for either head, tail or relationship in the triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Multitask_Tester(model, index_dataset, rel_set, entities_set, task='tail', norm_type='L2', batch_test=25):\n",
    "    \n",
    "    #computing sizes to provide to the embedding layers\n",
    "    rel_size=np.shape(rel_set)[0]\n",
    "    entities_size=np.shape(entities_set)[0]\n",
    "    \n",
    "    #dividing the dataset for testing\n",
    "    Test_entities_set=index_dataset.drop(['l'],1).to_numpy()  #Head [0], Tail[1]\n",
    "    Test_rel_set=index_dataset['l'].to_numpy() #Relationship\n",
    "\n",
    "    if task=='relationship':\n",
    "        \n",
    "        in_e=keras.Input(batch_shape=(batch_test,2))\n",
    "\n",
    "        #using a constant input for the relationships\n",
    "        in_r=keras.Input(tensor=K.constant(np.array(list(range(rel_size))*batch_test), shape=(batch_test,rel_size)))\n",
    "\n",
    "        embedding_e = layers.Embedding(entities_size, k, input_length=2)(in_e)\n",
    "        embedding_r = layers.Embedding(rel_size, k, input_length=rel_size)(in_r)\n",
    "\n",
    "        embedding   = layers.Concatenate(axis=1)([embedding_e,embedding_r])\n",
    "\n",
    "        h           = layers.Lambda( lambda y: y[:,0,:])(embedding)\n",
    "        t           = layers.Lambda( lambda y: y[:,1,:])(embedding)\n",
    "        l           = layers.Lambda( lambda y: y[:,2:,:])(embedding)\n",
    "\n",
    "        ents        = merge.subtract([h,t])\n",
    "\n",
    "        dist        = merge.add([l,ents])\n",
    "\n",
    "    elif task=='head':\n",
    "        \n",
    "        in_t=keras.Input(batch_shape=(batch_test,1))\n",
    "\n",
    "        #using a constant input for the heads\n",
    "        in_h=keras.Input(tensor=K.constant(np.array(list(range(entities_size))*batch_test), shape=(batch_test,entities_size)))\n",
    "\n",
    "        in_r=keras.Input(batch_shape=(batch_test,1))\n",
    "\n",
    "        embedding_r = layers.Embedding(rel_size, k, input_length=1)(in_r)\n",
    "        embedding_t = layers.Embedding(entities_size, k, input_length=1)(in_t)\n",
    "        embedding_h = layers.Embedding(entities_size, k, input_length=entities_size)(in_h)\n",
    "\n",
    "        embedding   = layers.Concatenate(axis=1)([embedding_r,embedding_t,embedding_h])\n",
    "\n",
    "        l           = layers.Lambda( lambda y: y[:,0,:])(embedding)\n",
    "        t           = layers.Lambda( lambda y: y[:,1,:])(embedding)\n",
    "        h           = layers.Lambda( lambda y: y[:,2:,:])(embedding)\n",
    "\n",
    "        rhs         = merge.subtract([l,t])\n",
    "\n",
    "        dist        = merge.add([h,rhs])\n",
    "    \n",
    "    elif task=='tail':\n",
    "        \n",
    "        in_h=keras.Input(batch_shape=(batch_test,1))\n",
    "\n",
    "        #using a constant input for the tails\n",
    "        in_t=keras.Input(tensor=K.constant(np.array(list(range(entities_size))*batch_test), shape=(batch_test,entities_size)))\n",
    "\n",
    "        in_r=keras.Input(batch_shape=(batch_test,1))\n",
    "\n",
    "        embedding_r = layers.Embedding(rel_size, k, input_length=1)(in_r)\n",
    "        embedding_h = layers.Embedding(entities_size, k, input_length=1)(in_h)\n",
    "        embedding_t = layers.Embedding(entities_size, k, input_length=entities_size)(in_t)\n",
    "\n",
    "        embedding   = layers.Concatenate(axis=1)([embedding_r,embedding_h,embedding_t])\n",
    "\n",
    "        l           = layers.Lambda( lambda y: y[:,0,:])(embedding)\n",
    "        h           = layers.Lambda( lambda y: y[:,1,:])(embedding)\n",
    "        t           = layers.Lambda( lambda y: y[:,2:,:])(embedding)\n",
    "\n",
    "        lhs         = merge.add([h,l])\n",
    "\n",
    "        dist        = merge.subtract([lhs,t])\n",
    "        \n",
    "    #computing the dissimilarity measure\n",
    "    \n",
    "    if norm_type=='L2':\n",
    "        res         = layers.Lambda( lambda y: K.pow(L2_norm(y,False),-1))(dist)\n",
    "\n",
    "    elif norm_type=='L1':\n",
    "        res         = layers.Lambda( lambda y: K.pow(L1_norm(y,False),-1))(dist)\n",
    "        \n",
    "        \n",
    "    #finalization of the model and actual testing \n",
    "    if task=='relationship':\n",
    "        \n",
    "        #sorting the outputs by increasing dissmilarity measure\n",
    "        sorted_data = layers.Lambda(lambda y: K.tf.nn.top_k(y, k=rel_size, sorted=True).indices)(res)\n",
    "        \n",
    "        #creating the model with the layers defined above\n",
    "        model_test=keras.Model([in_e,in_r], sorted_data)\n",
    "        \n",
    "        #copying the emebedding layers from the trained model\n",
    "        model_test.layers[2].set_weights(model.layers[2].get_weights())\n",
    "        model_test.layers[3].set_weights(model.layers[3].get_weights())\n",
    "        \n",
    "        #testing\n",
    "        n=np.shape(Test_rel_set)[0]\n",
    "    \n",
    "        ent=Test_entities_set.reshape(n,2)\n",
    "\n",
    "        y_true=Test_rel_set.reshape(n,1)\n",
    "\n",
    "        out=model_test.predict(ent, batch_size=batch_test)\n",
    "        ranks=np.argwhere(out==y_true)[:,-1]\n",
    "\n",
    "    elif task=='head':\n",
    "        \n",
    "        sorted_data = layers.Lambda(lambda y: K.tf.nn.top_k(y, k=entities_size , sorted=True).indices)(res)\n",
    "        model_test=keras.Model([in_r,in_h,in_t], sorted_data)\n",
    "        model_test.layers[3].set_weights(model.layers[3].get_weights())\n",
    "        model_test.layers[4].set_weights(model.layers[2].get_weights())\n",
    "        model_test.layers[5].set_weights(model.layers[2].get_weights())\n",
    "        \n",
    "        n=np.shape(Test_entities_set)[0]\n",
    "    \n",
    "        rel=Test_rel_set.reshape(n,1)\n",
    "        tails= Test_entities_set[:,1].reshape(n,1)\n",
    "\n",
    "        y_true=Test_entities_set[:,0].reshape(n,1)\n",
    "\n",
    "        out=model_test.predict([rel,tails], batch_size=batch_test)\n",
    "        ranks=np.argwhere(out==y_true)[:,-1]\n",
    "\n",
    "\n",
    "    elif task=='tail':\n",
    "        sorted_data = layers.Lambda(lambda y: K.tf.nn.top_k(y, k=entities_size , sorted=True).indices)(res)\n",
    "        model_test=keras.Model([in_r,in_t,in_h], sorted_data)\n",
    "        model_test.layers[3].set_weights(model.layers[3].get_weights())\n",
    "        model_test.layers[4].set_weights(model.layers[2].get_weights())\n",
    "        model_test.layers[5].set_weights(model.layers[2].get_weights())\n",
    "        \n",
    "        n=np.shape(Test_entities_set)[0]\n",
    "    \n",
    "        rel=Test_rel_set.reshape(n,1)\n",
    "        heads= Test_entities_set[:,0].reshape(n,1)\n",
    "\n",
    "        y_true=Test_entities_set[:,1].reshape(n,1)\n",
    "\n",
    "        out=model_test.predict([rel,heads], batch_size=batch_test)\n",
    "        ranks=np.argwhere(out==y_true)[:,-1]\n",
    "        \n",
    "    mean_rank=int(np.mean(ranks))\n",
    "    hit10=float(\"{:.2f}\".format(sum(ranks<10)*100/len(ranks)))\n",
    "    hit1=float(\"{:.2f}\".format(sum(ranks==0)*100/len(ranks)))\n",
    "        \n",
    "    return mean_rank, hit10, hit1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Main</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters' Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=50 #or 20\n",
    "eta=0.001 #or 0.01\n",
    "batch=100\n",
    "norm_type='L1' # or 'L2'\n",
    "optimizer='SGD' #or 'Adam'\n",
    "gamma=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing training set to create model with dummy embeddings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame(pd.read_csv(os.getcwd()+'/FB15k/freebase_mtr100_mte100-train.txt', sep='\\t', names=['h','l','t']))\n",
    "#getting all the entities in the dataset and creating related dataframe\n",
    "\n",
    "entities=pd.DataFrame(pd.DataFrame(np.hstack([dataset['h'], dataset['t']]))[0].unique(), columns=['entity']).reset_index()\n",
    "\n",
    "#getting all the relationships in the dataset and creating related dataframe\n",
    "\n",
    "rel=pd.DataFrame(dataset['l'].unique(), columns=['relationship']).reset_index()\n",
    "\n",
    "index_dataset=create_indexed_dataset(dataset, entities, rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating model with dummy embeddings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Francesco\\anaconda3\\envs\\datasheep\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Francesco\\anaconda3\\envs\\datasheep\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Francesco\\anaconda3\\envs\\datasheep\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Francesco\\anaconda3\\envs\\datasheep\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=training_model_creation(rel, entities, k, norm_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Francesco\\anaconda3\\envs\\datasheep\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Francesco\\anaconda3\\envs\\datasheep\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Francesco\\anaconda3\\envs\\datasheep\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Francesco\\anaconda3\\envs\\datasheep\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Francesco\\anaconda3\\envs\\datasheep\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Francesco\\anaconda3\\envs\\datasheep\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    \n",
    "    path=os.getcwd()+'/Results/BATCH'+str(batch)+'/'+norm_type+'/k'+str(k)+'/'+optimizer+'/'\n",
    "\n",
    "    directory=os.listdir(path)\n",
    "    for el in directory: \n",
    "        matching_e=re.findall('entities*.*'+str(eta)+'*.*', el)\n",
    "\n",
    "        if len(matching_e) != 0:\n",
    "            entities_emb=np.load(path+matching_e[0])\n",
    "\n",
    "        matching_r=re.findall('rel*.*'+str(eta)+'*.*', el)\n",
    "\n",
    "        if len(matching_r) != 0:\n",
    "            rel_emb=np.load(path+matching_r[0])\n",
    "            \n",
    "    model.layers[2].set_weights(entities_emb) #set entities embeddings\n",
    "    model.layers[3].set_weights(rel_emb) #set relationships emebeddings\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test=create_indexed_dataset(pd.read_csv(os.getcwd()+'/FB15k/freebase_mtr100_mte100-test.txt', sep='\\t', \n",
    "                                              names=['h','l','t']), entities, rel).drop('index',1).head(19000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test=19 #test set size must be a multiple of test batch size\n",
    "\n",
    "R=Multitask_Tester(model, index_test, rel, entities,\n",
    "                   task='relationship', norm_type=norm_type, batch_test=batch_test)\n",
    "H=Multitask_Tester(model, index_test, rel, entities,\n",
    "                   task='head', norm_type=norm_type, batch_test=batch_test)\n",
    "T=Multitask_Tester(model, index_test, rel, entities,\n",
    "                   task='tail', norm_type=norm_type, batch_test=batch_test)\n",
    "Test_Res=[R,H,T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tMean Rank\tHit@10\tHit@1\n",
      "Entity prediction\t: [159.5    41.73    7.695]\n",
      "Relationship prediction\t: (33, 74.33, 20.83)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\t\\tMean Rank\\tHit@10\\tHit@1\")\n",
    "print('Entity prediction\\t:',np.mean(Test_Res[1:], axis=0)) \n",
    "print('Relationship prediction\\t:',Test_Res[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasheep",
   "language": "python",
   "name": "datasheep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1030px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
