{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from functools import partial\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers \n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Functions for the training model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_init(k,shape, dtype=None):\n",
    "    unnormed = K.random_uniform(shape=shape, minval=-6/np.sqrt(k), maxval=6/np.sqrt(k), dtype=dtype)\n",
    "    norm     = K.sqrt(K.sum(K.pow(unnormed,2), axis=-1, keepdims=True))  \n",
    "    return unnormed/norm\n",
    "\n",
    "def L2_norm(x, keepdims=True):\n",
    "    return K.sqrt(K.sum(K.pow(x,2), axis=-1, keepdims=keepdims))\n",
    "\n",
    "def L1_norm(x, keepdims=True):\n",
    "    return K.sum(K.abs(x), axis=-1, keepdims=keepdims)\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):  ## y_true is unused, but keras \"fit\" method requires it, so when calling\n",
    "    return K.sum(y_pred)            ## the \"fit\" method also the y argument must be passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset manipulation functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexed_dataset(dataset, entities, rel):\n",
    "    #substituting the entities and the relationships in the dataset with their respective index in the \n",
    "    #entities dataframe and relationships dataframe\n",
    "\n",
    "    index_dataset=pd.merge(dataset, entities.rename(columns={\"entity\": \"h\"}), \n",
    "                           on='h', left_index=True).drop('h',1).rename(columns={\"index\": \"h\"})\n",
    "    index_dataset=pd.merge(index_dataset, entities.rename(columns={\"entity\": \"t\"}), \n",
    "                           on='t', left_index=True).drop('t',1).rename(columns={\"index\": \"t\"})\n",
    "    index_dataset=pd.merge(index_dataset, rel.rename(columns={\"relationship\": \"l\"}), \n",
    "                           on='l', left_index=True).drop('l',1).rename(\n",
    "                           columns={\"index\": \"l\"}).reset_index().drop('index',1).reset_index()\n",
    "    return index_dataset\n",
    "\n",
    "\n",
    "def generate_pos_neg_set(index_dataset, more_fake_negatives=None, check_triplets=False):\n",
    "   \n",
    "    Negative=index_dataset.copy()\n",
    "\n",
    "    entities_size=np.shape(model.layers[2].get_weights()[0])[0]\n",
    "    #generate negative triplets\n",
    "\n",
    "    Negative_h,Negative_t=np.split(shuffle(Negative, random_state=np.random.randint(0,10000)), 2)\n",
    "    Negative_h['h']=Negative_h['h'].apply(lambda x: np.random.randint(0,entities_size))\n",
    "    Negative_t['t']=Negative_t['t'].apply(lambda x: np.random.randint(0,entities_size))\n",
    "    Negative=pd.concat((Negative_h, Negative_t))\n",
    "\n",
    "    if check_triplets==True:\n",
    "        #remove fake negative triplets\n",
    "        if more_fake_negatives is not None:\n",
    "            Good_Triplets=pd.concat((index_dataset, more_fake_negatives))\n",
    "            Negative = pd.merge(Good_Triplets.drop('index',1), Negative, on=['h', 't', 'l'], how='right', indicator='Exist')\n",
    "        else:\n",
    "            Negative = pd.merge(index_dataset.drop('index',1), Negative, on=['h', 't', 'l'], how='right', indicator='Exist')\n",
    "        Negative = Negative.drop(Negative[Negative['Exist']=='both'].index, axis=0).drop('Exist',1).rename(columns={\"h\": \"h1\", \"t\": \"t1\"})\n",
    "        #merge positive and negative triplets\n",
    "        Total=pd.merge(index_dataset, Negative, on='index').drop(['l_y','index'],1).rename(columns={\"l_x\": \"l\"})\n",
    "    else:\n",
    "    #merge positive and negative triplets\n",
    "        Negative= Negative.rename(columns={\"h\": \"h1\", \"t\": \"t1\"})\n",
    "        Total=pd.merge(index_dataset, Negative, on='index').drop(['l_y','index'],1).rename(columns={\"l_x\": \"l\"})\n",
    "\n",
    "    Entities_set=Total.drop(['l'],1).to_numpy()\n",
    "    Rel_set=Total['l'].to_numpy()\n",
    "    n=np.shape(Rel_set)[0]\n",
    "\n",
    "    e=Entities_set.reshape(n,4)\n",
    "    l=Rel_set.reshape(n,1)\n",
    "    \n",
    "    return [e,l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model_creation(rel_set, entities_set, k, norm_type='L2', optimizer='sgd'):\n",
    "\n",
    "    #get the sizes of the sets of unique relationships and entities\n",
    "    \n",
    "    rel_size=np.shape(rel_set)[0]\n",
    "    entities_size=np.shape(entities_set)[0]\n",
    "    \n",
    "    #Define initializers for the embedings\n",
    "    \n",
    "    init=keras.initializers.RandomUniform(minval=-6/np.sqrt(k), maxval=6/np.sqrt(k))\n",
    "    rel_initializer=partial(rel_init,k)\n",
    "    \n",
    "    #Define norm constraint on entities' embeddings\n",
    "    norm=keras.constraints.UnitNorm(axis=1)\n",
    "\n",
    "    #Define input shape\n",
    "    in_e=keras.Input((4,), name='entities')\n",
    "    in_r=keras.Input((1,), name='relationship')\n",
    "\n",
    "    #Define embedding layers\n",
    "    embedding_e = layers.Embedding(entities_size, k, input_length=4, embeddings_initializer=init, embeddings_constraint=norm, name='entities_embedding')(in_e)\n",
    "    embedding_r = layers.Embedding(rel_size, k, input_length=1, embeddings_initializer=rel_initializer, name='relationships_embedding')(in_r)\n",
    "\n",
    "    #Concatenate embedding layers into one layer\n",
    "    embedding   = layers.Concatenate(axis=1)([embedding_e,embedding_r])\n",
    "\n",
    "    #Find the values for the triplets (h,l,t) (positive) and (h1,l,t1) (negative)\n",
    "    \n",
    "    h           = layers.Lambda( lambda y: y[:,0,:])(embedding)\n",
    "    t           = layers.Lambda( lambda y: y[:,1,:])(embedding)\n",
    "    h1          = layers.Lambda( lambda y: y[:,2,:])(embedding)\n",
    "    t1          = layers.Lambda( lambda y: y[:,3,:])(embedding)\n",
    "    l           = layers.Lambda( lambda y: y[:,4,:])(embedding)\n",
    "\n",
    "    pos         = layers.subtract([layers.add([h,l]),t])\n",
    "    neg         = layers.subtract([layers.add([h1,l]),t1])\n",
    "\n",
    "    #Compute the dissimilarities/energies of the two triplets...\n",
    "\n",
    "    if norm_type=='L2':\n",
    "        pos_mid     = layers.Lambda(L2_norm)(pos) #L2 norm\n",
    "        neg_mid     = layers.Lambda(L2_norm)(neg) #L2 norm\n",
    "\n",
    "    elif norm_type=='L1':\n",
    "        pos_mid     = layers.Lambda(L1_norm)(pos) #L1 norm\n",
    "        neg_mid     = layers.Lambda(L1_norm)(neg) #L1 norm\n",
    "\n",
    "    #...then subtract them\n",
    "    \n",
    "    sub         = layers.subtract([pos_mid,neg_mid])\n",
    "\n",
    "    #finally compute [\\gamma+d(positive_triplet)-d(negative_triplet)]_+\n",
    "    \n",
    "    out         = layers.Lambda(lambda y: K.maximum(y+gamma,0))(sub)\n",
    "\n",
    "    \n",
    "    model=keras.Model([in_e,in_r], out)\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function for training execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_embeddings(model, training_set, epochs, batch_size, epochs_val_loss=10, validation_set=None, check_triplets=False):\n",
    "    \n",
    "    if validation_set is not None:\n",
    "        e_val,l_val=generate_pos_neg_set(training_set,validation_set)\n",
    "        best_loss=1000 #big number\n",
    "        best_model=None\n",
    "        best_epoch=0\n",
    "        patience=0\n",
    "        \n",
    "    for ep in range(epochs):\n",
    "        \n",
    "        print('\\nEpoch:\\t'+str(ep+1)+'/'+str(epochs), end='\\t')\n",
    "        start=time.time()\n",
    "        \n",
    "        if validation_set is not None:\n",
    "            e,l=generate_pos_neg_set(training_set,validation_set, check_triplets=check_triplets)\n",
    "            \n",
    "        else:\n",
    "            e,l=generate_pos_neg_set(training_set,check_triplets=check_triplets)\n",
    "        \n",
    "        model.fit([e,l],l*0, batch_size=batch_size, epochs=1, verbose=0)\n",
    "        stop=time.time()\n",
    "        print('Time elapsed:\\t{:.3f} s'.format(stop-start))\n",
    "        \n",
    "        \n",
    "        if validation_set is not None and ep%epochs_val_loss==0:\n",
    "            \n",
    "            #save model with best score on validation set\n",
    "            \n",
    "            loss=model.evaluate([e_val,l_val],l_val*0, batch_size=batch_size, verbose=0)\n",
    "            \n",
    "            \n",
    "            if loss<best_loss:\n",
    "                best_loss=loss\n",
    "                best_epoch=ep\n",
    "                best_model=model\n",
    "                patience=0\n",
    "            \n",
    "            else:\n",
    "                patience+=1\n",
    "            \n",
    "            if patience>9:\n",
    "                break\n",
    "            \n",
    "    if validation_set is not None:\n",
    "        print(\"\\nBest validation score at epoch {}. Score:{}\".format(best_epoch, best_loss))\n",
    "        model=best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function for test model definition and execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The function performs prediction for either head, tail or relationship in the triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Multitask_Tester(model, index_dataset, rel_set, entities_set, task='tail', norm_type='L2', batch_test=25):\n",
    "    \n",
    "    #computing sizes to provide to the embedding layers\n",
    "    rel_size=np.shape(rel_set)[0]\n",
    "    entities_size=np.shape(entities_set)[0]\n",
    "    \n",
    "    #dividing the dataset for testing\n",
    "    Test_entities_set=index_dataset.drop(['l'],1).to_numpy()  #Head [0], Tail[1]\n",
    "    Test_rel_set=index_dataset['l'].to_numpy() #Relationship\n",
    "\n",
    "    if task=='relationship':\n",
    "        \n",
    "        in_e=keras.Input(batch_shape=(batch_test,2))\n",
    "\n",
    "        #using a constant input for the relationships\n",
    "        in_r=keras.Input(tensor=K.constant(np.array(list(range(rel_size))*batch_test), shape=(batch_test,rel_size)))\n",
    "\n",
    "        embedding_e = layers.Embedding(entities_size, k, input_length=2)(in_e)\n",
    "        embedding_r = layers.Embedding(rel_size, k, input_length=rel_size)(in_r)\n",
    "\n",
    "        embedding   = layers.Concatenate(axis=1)([embedding_e,embedding_r])\n",
    "\n",
    "        h           = layers.Lambda( lambda y: y[:,0,:])(embedding)\n",
    "        t           = layers.Lambda( lambda y: y[:,1,:])(embedding)\n",
    "        l           = layers.Lambda( lambda y: y[:,2:,:])(embedding)\n",
    "\n",
    "        ents        = layers.subtract([h,t])\n",
    "\n",
    "        dist        = layers.add([l,ents])\n",
    "\n",
    "    elif task=='head':\n",
    "        \n",
    "        in_t=keras.Input(batch_shape=(batch_test,1))\n",
    "\n",
    "        #using a constant input for the heads\n",
    "        in_h=keras.Input(tensor=K.constant(np.array(list(range(entities_size))*batch_test), shape=(batch_test,entities_size)))\n",
    "\n",
    "        in_r=keras.Input(batch_shape=(batch_test,1))\n",
    "\n",
    "        embedding_r = layers.Embedding(rel_size, k, input_length=1)(in_r)\n",
    "        embedding_t = layers.Embedding(entities_size, k, input_length=1)(in_t)\n",
    "        embedding_h = layers.Embedding(entities_size, k, input_length=entities_size)(in_h)\n",
    "\n",
    "        embedding   = layers.Concatenate(axis=1)([embedding_r,embedding_t,embedding_h])\n",
    "\n",
    "        l           = layers.Lambda( lambda y: y[:,0,:])(embedding)\n",
    "        t           = layers.Lambda( lambda y: y[:,1,:])(embedding)\n",
    "        h           = layers.Lambda( lambda y: y[:,2:,:])(embedding)\n",
    "\n",
    "        rhs         = layers.subtract([l,t])\n",
    "\n",
    "        dist        = layers.add([h,rhs])\n",
    "    \n",
    "    elif task=='tail':\n",
    "        \n",
    "        in_h=keras.Input(batch_shape=(batch_test,1))\n",
    "\n",
    "        #using a constant input for the tails\n",
    "        in_t=keras.Input(tensor=K.constant(np.array(list(range(entities_size))*batch_test), shape=(batch_test,entities_size)))\n",
    "\n",
    "        in_r=keras.Input(batch_shape=(batch_test,1))\n",
    "\n",
    "        embedding_r = layers.Embedding(rel_size, k, input_length=1)(in_r)\n",
    "        embedding_h = layers.Embedding(entities_size, k, input_length=1)(in_h)\n",
    "        embedding_t = layers.Embedding(entities_size, k, input_length=entities_size)(in_t)\n",
    "\n",
    "        embedding   = layers.Concatenate(axis=1)([embedding_r,embedding_h,embedding_t])\n",
    "\n",
    "        l           = layers.Lambda( lambda y: y[:,0,:])(embedding)\n",
    "        h           = layers.Lambda( lambda y: y[:,1,:])(embedding)\n",
    "        t           = layers.Lambda( lambda y: y[:,2:,:])(embedding)\n",
    "\n",
    "        lhs         = layers.add([h,l])\n",
    "\n",
    "        dist        = layers.subtract([lhs,t])\n",
    "        \n",
    "    #computing the dissimilarity measure\n",
    "    \n",
    "    if norm_type=='L2':\n",
    "        res         = layers.Lambda( lambda y: K.pow(L2_norm(y,False),-1))(dist)\n",
    "\n",
    "    elif norm_type=='L1':\n",
    "        res         = layers.Lambda( lambda y: K.pow(L1_norm(y,False),-1))(dist)\n",
    "        \n",
    "        \n",
    "    #finalization of the model and actual testing \n",
    "    if task=='relationship':\n",
    "        \n",
    "        #sorting the outputs by increasing dissmilarity measure\n",
    "        sorted_data = layers.Lambda(lambda y: K.tf.nn.top_k(y, k=rel_size, sorted=True).indices)(res)\n",
    "        \n",
    "        #creating the model with the layers defined above\n",
    "        model_test=keras.Model([in_e,in_r], sorted_data)\n",
    "        \n",
    "        #copying the emebedding layers from the trained model\n",
    "        model_test.layers[2].set_weights(model.layers[2].get_weights())\n",
    "        model_test.layers[3].set_weights(model.layers[3].get_weights())\n",
    "        \n",
    "        #testing\n",
    "        n=np.shape(Test_rel_set)[0]\n",
    "    \n",
    "        ent=Test_entities_set.reshape(n,2)\n",
    "\n",
    "        y_true=Test_rel_set.reshape(n,1)\n",
    "\n",
    "        out=model_test.predict(ent, batch_size=batch_test)\n",
    "        ranks=np.argwhere(out==y_true)[:,-1]\n",
    "\n",
    "    elif task=='head':\n",
    "        \n",
    "        sorted_data = layers.Lambda(lambda y: K.tf.nn.top_k(y, k=entities_size , sorted=True).indices)(res)\n",
    "        model_test=keras.Model([in_r,in_h,in_t], sorted_data)\n",
    "        model_test.layers[3].set_weights(model.layers[3].get_weights())\n",
    "        model_test.layers[4].set_weights(model.layers[2].get_weights())\n",
    "        model_test.layers[5].set_weights(model.layers[2].get_weights())\n",
    "        \n",
    "        n=np.shape(Test_entities_set)[0]\n",
    "    \n",
    "        rel=Test_rel_set.reshape(n,1)\n",
    "        tails= Test_entities_set[:,1].reshape(n,1)\n",
    "\n",
    "        y_true=Test_entities_set[:,0].reshape(n,1)\n",
    "\n",
    "        out=model_test.predict([rel,tails], batch_size=batch_test)\n",
    "        ranks=np.argwhere(out==y_true)[:,-1]\n",
    "\n",
    "\n",
    "    elif task=='tail':\n",
    "        sorted_data = layers.Lambda(lambda y: K.tf.nn.top_k(y, k=entities_size , sorted=True).indices)(res)\n",
    "        model_test=keras.Model([in_r,in_t,in_h], sorted_data)\n",
    "        model_test.layers[3].set_weights(model.layers[3].get_weights())\n",
    "        model_test.layers[4].set_weights(model.layers[2].get_weights())\n",
    "        model_test.layers[5].set_weights(model.layers[2].get_weights())\n",
    "        \n",
    "        n=np.shape(Test_entities_set)[0]\n",
    "    \n",
    "        rel=Test_rel_set.reshape(n,1)\n",
    "        heads= Test_entities_set[:,0].reshape(n,1)\n",
    "\n",
    "        y_true=Test_entities_set[:,1].reshape(n,1)\n",
    "\n",
    "        out=model_test.predict([rel,heads], batch_size=batch_test)\n",
    "        ranks=np.argwhere(out==y_true)[:,-1]\n",
    "        \n",
    "    mean_rank=int(np.mean(ranks))\n",
    "    hit10=float(\"{:.2f}\".format(sum(ranks<10)*100/len(ranks)))\n",
    "    hit1=float(\"{:.2f}\".format(sum(ranks==0)*100/len(ranks)))\n",
    "        \n",
    "    return mean_rank, hit10, hit1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Main</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameters' Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=50\n",
    "eta=0.01\n",
    "gamma=1\n",
    "epochs=2\n",
    "batch=300\n",
    "norm_type='L1' # or 'L2'\n",
    "\n",
    "#set the optimizer\n",
    "SGD=optimizers.SGD(lr=eta)\n",
    "#Adam=optimizers.Adam(lr=eta) #eta should be smaller than 0.001 to obtain decent results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing the Training set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame(pd.read_csv(os.getcwd()+'/FB15k/freebase_mtr100_mte100-train.txt', sep='\\t', names=['h','l','t']))\n",
    "#getting all the entities in the dataset and creating related dataframe\n",
    "\n",
    "entities=pd.DataFrame(pd.DataFrame(np.hstack([dataset['h'], dataset['t']]))[0].unique(), columns=['entity']).reset_index()\n",
    "\n",
    "#getting all the relationships in the dataset and creating related dataframe\n",
    "\n",
    "rel=pd.DataFrame(dataset['l'].unique(), columns=['relationship']).reset_index()\n",
    "\n",
    "index_dataset=create_indexed_dataset(dataset, entities, rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing the validation set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_val=create_indexed_dataset(pd.read_csv(os.getcwd()+'/FB15k/freebase_mtr100_mte100-valid.txt', sep='\\t', \n",
    "                                             names=['h','l','t']), entities, rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Learning the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesi=model.layers[10].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(pesi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:\t1/2\t"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "in user code:\n\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:756 train_step\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2743 _minimize\n        optimizer.apply_gradients(\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:545 apply_gradients\n        return distribute_ctx.get_replica_context().merge_call(\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2715 merge_call\n        return self._merge_call(merge_fn, args, kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2722 _merge_call\n        return merge_fn(self._strategy, *args, **kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:632 _distributed_apply  **\n        update_ops.extend(distribution.extended.update(\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2300 update\n        return self._update(var, fn, args, kwargs, group)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2955 _update\n        return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2961 _update_non_slot\n        result = fn(*args, **kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:599 apply_grad_to_update_var  **\n        raise RuntimeError(\n\n    RuntimeError: Cannot use a constraint function on a sparse variable.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3433742dc3e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_model_creation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m learn_embeddings(model, training_set=index_dataset, epochs=epochs, \n\u001b[0m\u001b[0;32m      3\u001b[0m                  batch_size=batch, validation_set=index_val, check_triplets=True)\n",
      "\u001b[1;32m<ipython-input-6-64882ed5f5fb>\u001b[0m in \u001b[0;36mlearn_embeddings\u001b[1;34m(model, training_set, epochs, batch_size, epochs_val_loss, validation_set, check_triplets)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerate_pos_neg_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheck_triplets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_triplets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time elapsed:\\t{:.3f} s'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: in user code:\n\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:756 train_step\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2743 _minimize\n        optimizer.apply_gradients(\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:545 apply_gradients\n        return distribute_ctx.get_replica_context().merge_call(\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2715 merge_call\n        return self._merge_call(merge_fn, args, kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2722 _merge_call\n        return merge_fn(self._strategy, *args, **kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:632 _distributed_apply  **\n        update_ops.extend(distribution.extended.update(\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2300 update\n        return self._update(var, fn, args, kwargs, group)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2955 _update\n        return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2961 _update_non_slot\n        result = fn(*args, **kwargs)\n    C:\\Users\\Francesco\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:599 apply_grad_to_update_var  **\n        raise RuntimeError(\n\n    RuntimeError: Cannot use a constraint function on a sparse variable.\n"
     ]
    }
   ],
   "source": [
    "model=training_model_creation(rel, entities, k, norm_type, SGD)\n",
    "learn_embeddings(model, training_set=index_dataset, epochs=epochs, \n",
    "                 batch_size=batch, validation_set=index_val, check_triplets=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_test=create_indexed_dataset(pd.read_csv(os.getcwd()+'/FB15k/freebase_mtr100_mte100-test.txt', sep='\\t', \n",
    "                                              names=['h','l','t']), entities, rel).drop('index',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test=19 #test set size must be a multiple of test batch size\n",
    "\n",
    "R=Multitask_Tester(model, index_test, rel, entities,\n",
    "                   task='relationship', norm_type=norm_type, batch_test=batch_test)\n",
    "H=Multitask_Tester(model, index_test, rel, entities,\n",
    "                   task='head', norm_type=norm_type, batch_test=batch_test)\n",
    "T=Multitask_Tester(model, index_test, rel, entities,\n",
    "                   task='tail', norm_type=norm_type, batch_test=batch_test)\n",
    "Test_Res=[R,H,T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t\\tMean Rank\\tHit@10\\tHit@1\")\n",
    "print('Entity prediction\\t:',np.mean(Test_Res[1:], axis=0)) \n",
    "print('Relationship prediction\\t:',Test_Res[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('entities', model.layers[2].get_weights())\n",
    "np.save('rel', model.layers[3].get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
